{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset import *\n",
    "from sklearn.linear_model import LinearRegression as LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_simple,t_simple = simple_dataset(noise=10,n_samples=300)\n",
    "\n",
    "## feature normalisation\n",
    "\n",
    "# X_simple = X_simple / X_simple.max()\n",
    "\n",
    "division_ratio = 0.8\n",
    "split_point =int(division_ratio * len(X_simple)) \n",
    "\n",
    "X_train_simple = X_simple[:split_point]\n",
    "X_test_simple = X_simple[split_point:]\n",
    "\n",
    "t_train_simple = t_simple[:split_point]\n",
    "t_test_simple = t_simple[split_point:]\n",
    "\n",
    "\n",
    "X_complex,t_complex = remodel_complex_dataset()\n",
    "\n",
    "split_point =int(division_ratio * len(X_complex)) \n",
    "\n",
    "X_train_complex = X_complex[:split_point]\n",
    "X_test_complex = X_complex[split_point:]\n",
    "\n",
    "t_train_complex = t_complex[:split_point]\n",
    "t_test_complex = t_complex[split_point:]\n",
    "\n",
    "\n",
    "X_complex_default,t_complex_default = complex_dataset()\n",
    "# X_complex_default = np.reshape(X_complex_default,X_complex_default.shape[0])\n",
    "X_complex_default = X_complex_default[:,np.newaxis]\n",
    "\n",
    "split_point =int(division_ratio * len(X_complex_default))\n",
    "\n",
    "X_train_complex_default = X_complex_default[:split_point]\n",
    "X_test_complex_default = X_complex_default[split_point:]\n",
    "\n",
    "t_train_complex_default = t_complex_default[:split_point]\n",
    "t_test_complex_default = t_complex_default[split_point:]\n",
    "\n",
    "\n",
    "print(X_simple.dtype)\n",
    "\n",
    "print(X_train_simple.shape)\n",
    "print(t_train_simple.shape)\n",
    "\n",
    "print(X_train_complex.shape)\n",
    "print(t_train_complex.shape)\n",
    "\n",
    "print(X_test_complex.shape)\n",
    "print(t_test_complex.shape)\n",
    "\n",
    "print(X_complex_default.shape,t_complex_default.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(object):\n",
    "\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000, eps=1e-6):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.eps = eps\n",
    "        ## [(epoch,error)]\n",
    "        self.errors = []\n",
    "\n",
    "    def fit(self, X, t):\n",
    "\n",
    "        N, D = X.shape\n",
    "        \n",
    "        self.w = np.zeros(D)\n",
    "        prev_w = self.w \n",
    "        prev_cost = self.cost(X,t)\n",
    "\n",
    "        for ep in range(self.epochs):\n",
    "            g = self.gradient(X,t)\n",
    "            step = self.learning_rate * g\n",
    "            self.w = self.w - step\n",
    "            \n",
    "            prev_cost = self.cost(X,t)\n",
    "\n",
    "            self.errors += [(ep,prev_cost)]\n",
    "\n",
    "            if np.linalg.norm(self.w - prev_w) < self.eps:\n",
    "                # print(\"Tolerance broke for: \",self.w,prev_w)\n",
    "                break\n",
    "\n",
    "            prev_w = self.w\n",
    "\n",
    "        # print(\"Done with all epochs\")\n",
    "\n",
    "        # return self\n",
    "\n",
    "    def predict(self, X, return_std=False):\n",
    "\n",
    "        N, D = X.shape\n",
    "\n",
    "        y = X @ self.w\n",
    "        # print(X.shape,y.shape,\"IN predict\")\n",
    "        \n",
    "        return y\n",
    "\n",
    "    \n",
    "    ## Not 1 / n \n",
    "    def cost(self,X,t,return_std=False):\n",
    "        y = self.predict(X,return_std)\n",
    "        loss = (y - t) ** 2\n",
    "        # return 0.5 * loss\n",
    "        return np.mean(loss)\n",
    "\n",
    "    def gradient(self,X,t):\n",
    "        y = self.predict(X)\n",
    "        # print(\"In gradient X is: \", X.shape,\"T is: \",t.shape,\"Y is: \",y.shape,\" Modified is: \",y.reshape(-1,1).shape )\n",
    "        d_bias =  -2*sum(t - y)\n",
    "        # print(\"D_BIAS:\",d_bias)\n",
    "        d_w = -2*sum(X[:,1:] * (t - y).reshape(-1,1))\n",
    "        # print(\"D_W:\",d_w,d_w.dtype)\n",
    "        g = np.append(np.array(d_bias), d_w) \n",
    "        return g / X.shape[0] \n",
    "        # return g / 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LinearRegressionCompleteForm(object):\n",
    "\n",
    "    def fit(self, X, t):\n",
    "\n",
    "        N, D = X.shape\n",
    "        \n",
    "        self.w = np.linalg.pinv(X) @ t\n",
    "        # print(X.shape,t.shape,\"IN FIT\")\n",
    "        self.var = np.mean(np.square(X @ self.w - t))\n",
    "\n",
    "    def predict(self, X, return_std=False):\n",
    "\n",
    "        N, D = X.shape\n",
    "\n",
    "        y = X @ self.w\n",
    "        # print(X.shape,y.shape,\"IN predict\")\n",
    "        \n",
    "        if return_std:\n",
    "\n",
    "            y_std = np.sqrt(self.var) \n",
    "\n",
    "            return y, y_std\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_polynomial_features(X, M):\n",
    "    phi = X\n",
    "    return np.array([x ** np.arange(M + 1) for x in phi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y, t):\n",
    "    return np.mean((y - t) * (y- t))\n",
    "\n",
    "def normalised_squared_error(y,t):\n",
    "    return np.mean((y - t) / t.max()  * (y- t) / t.max())\n",
    "\n",
    "def RMSE(y,t):\n",
    "    return np.sqrt(mean_squared_error(y,t))\n",
    "\n",
    "def NRMSE(y,t):\n",
    "    return RMSE(y,t) / (np.max(t) - np.min(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On the simple dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t_train_simple.shape)\n",
    "## merge pana la grad 7 pana da overflow\n",
    "M = 1\n",
    "\n",
    "X_train_feat = extract_polynomial_features(X_train_simple,M) \n",
    "X_test_feat = extract_polynomial_features(X_test_simple,M)\n",
    "\n",
    "## needs a different lr for every configuration of simple input set\n",
    "model = LinearRegression(epochs=400,learning_rate=0.015)\n",
    "model.fit(X_train_feat,t_train_simple)\n",
    "y_test = model.predict(X_test_feat)\n",
    "\n",
    "sk_model = LR()\n",
    "sk_model.fit(X_train_simple,t_train_simple)\n",
    "\n",
    "y_test_sk = sk_model.predict(X_test_simple)\n",
    "\n",
    "complete_model = LinearRegressionCompleteForm()\n",
    "complete_model.fit(X_train_feat,t_train_simple)\n",
    "\n",
    "y_test_complete = complete_model.predict(X_test_feat)\n",
    "print(y_test_complete.shape)\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "plt.scatter(X_test_simple, t_test_simple, facecolor=\"none\", edgecolor=\"r\", s=50, label=\"simple testing data\")\n",
    "plt.plot(X_test_simple,y_test,label='Gradiend descent prediction')\n",
    "plt.plot(X_test_simple,y_test_sk,label='skitlearn prediction',color='g')\n",
    "plt.plot(X_test_simple,y_test_complete,label='Complete form',color='y')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plot_error_x = [ep[0] for ep in model.errors]\n",
    "plot_error_y = [ep[1] for ep in model.errors]\n",
    "\n",
    "plt.plot(plot_error_x,plot_error_y,label='Errors')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On the complex dataset with feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t_train_simple.shape)\n",
    "M = 1\n",
    "\n",
    "X_train_feat = X_train_complex \n",
    "X_test_feat = X_test_complex\n",
    "\n",
    "model = LinearRegression(learning_rate=0.001)\n",
    "model.fit(X_train_feat,t_train_complex)\n",
    "y_test = model.predict(X_test_feat)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_test_complex_default, t_test_complex_default, facecolor=\"none\", edgecolor=\"r\", s=50, label=\"complex testing data\")\n",
    "min_len = min(X_test_complex_default.shape[0],y_test.shape[0])\n",
    "plt.plot(X_test_complex_default[:min_len],y_test[:min_len],label='prediction')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.plot([ep[0] for ep in model.errors],[ep[1] for ep in model.errors],label='Errors')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression with gradient descent on the complex dataset without feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t_train_simple.shape)\n",
    "M = 1\n",
    "\n",
    "X_train_feat = extract_polynomial_features(X_train_complex_default,M) \n",
    "X_test_feat = extract_polynomial_features(X_test_complex_default,M)\n",
    "\n",
    "model = LinearRegression(learning_rate=0.01)\n",
    "model.fit(X_train_feat,t_train_complex_default)\n",
    "y_test = model.predict(X_test_feat)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_test_complex_default, t_test_complex_default, facecolor=\"none\", edgecolor=\"r\", s=50, label=\"complex testing data\")\n",
    "min_len = min(X_test_complex_default.shape[0],y_test.shape[0])\n",
    "plt.plot(X_test_complex_default[:min_len],y_test[:min_len],label='prediction')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.plot([ep[0] for ep in model.errors],[ep[1] for ep in model.errors],label='Errors')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between the complete form and gradient descent  on the simple dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 1\n",
    "X_train_feat = extract_polynomial_features(X_train_simple,M) \n",
    "X_test_feat = extract_polynomial_features(X_test_simple,M)\n",
    "\n",
    "model = LinearRegression(learning_rate=0.001)\n",
    "model.fit(X_train_feat,t_train_simple)\n",
    "y_test = model.predict(X_test_feat)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.xlabel(\"true output\")\n",
    "plt.ylabel(\"predicted output\")\n",
    "plt.scatter(t_test_simple,y_test_complete,label='Complete form',facecolor=\"none\", edgecolor=\"r\", s=50)\n",
    "plt.scatter(t_test_simple,y_test,label='Gradient descent form',color='g')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent learning rate analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_lr = {}\n",
    "X_train_feat = extract_polynomial_features(X_train_simple,1) \n",
    "X_test_feat = extract_polynomial_features(X_test_simple,1)\n",
    "\n",
    "## nu stiu daca trebuie X_feat @ X_feat.T sau X normal\n",
    "\n",
    "w,v = np.linalg.eigh(X_train_feat.T @ X_train_feat)\n",
    "eigen_lr = 1.0 / w.max()\n",
    "\n",
    "lrs = [eigen_lr,0.001,0.0001,1.00e-5]\n",
    "not_convergin_lrs = [1.00e-3,1.00e-2,1.00e-1,0.2,0.5]\n",
    "\n",
    "lrs += not_convergin_lrs\n",
    "\n",
    "for _lr in lrs:\n",
    "    model = LinearRegression(learning_rate=_lr,eps=1e-7)\n",
    "    model.fit(X_train_feat,t_train_simple)\n",
    "    y = model.predict(X_test_feat)\n",
    "    dict_lr[_lr] = model.errors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## some reach tolerance before others\n",
    "## other option is to pad with 0 to the max epoch number\n",
    "min_no_epochs = min([len(err) for err in dict_lr.values()])\n",
    "max_no_epochs = max([len(err) for err in dict_lr.values()])\n",
    "print(\"Min no of epochs:\",min_no_epochs)\n",
    "print(\"Max no of epochs:\",max_no_epochs)\n",
    "\n",
    "MAX_NO_EPOCHS = 1000\n",
    "\n",
    "min_no_epochs = min([len(err) for err in dict_lr.values()])\n",
    "max_no_epochs = max([len(err) for err in dict_lr.values()])\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "\n",
    "for _lr in dict_lr:\n",
    "    err = dict_lr[_lr]\n",
    "    plot_error_x = [ep[0] for ep in err if ep[0] <= 1000]\n",
    "    plot_error_y = [ep[1] for ep in err if ep[0] <= 1000]\n",
    "    plt.scatter(plot_error_x,plot_error_y,label=\"Learning rate= \" + str(_lr))\n",
    "    plt.plot(plot_error_x,plot_error_y)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Se observa ca pentru un learning step foarte mic (1e-5), algoritmul abia incepe sa convearga in 1000 de epoci. Pentru lr = max(eiegenvalue) algoritmul ajunge la convergenta in 400 de epoci. Cele mai bune rezultate se obtin pentru un lr mare (0.2, 0.1, 0.5) pe acest dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionMB(object):\n",
    "\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000, eps=1e-6):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.eps = eps\n",
    "        ## [(epoch,error)]\n",
    "        self.errors = []\n",
    "        self.batch_errors = [(0,10)]\n",
    "\n",
    "    def fit(self, X, t,batch_size=32,reshuffle=False):\n",
    "\n",
    "        N, D = X.shape\n",
    "        new_t = t[:,np.newaxis]\n",
    "        \n",
    "        self.w = np.zeros(D)\n",
    "        prev_w = self.w \n",
    "        prev_cost = self.cost(X,t)\n",
    "\n",
    "        if not reshuffle:\n",
    "            mini_batches = self.create_mini_batches(X, new_t, batch_size)\n",
    "        for ep in range(self.epochs):\n",
    "            if reshuffle:\n",
    "                mini_batches = self.create_mini_batches(X, new_t, batch_size)\n",
    "                \n",
    "            for mini_batch in mini_batches:\n",
    "                X_mini, t_mini = mini_batch\n",
    "\n",
    "                t_mini = t_mini[:,0]\n",
    "                \n",
    "                # print(X_mini.shape)\n",
    "                if X_mini.shape[0] == 0:\n",
    "                    # print(X_mini)\n",
    "                    continue\n",
    "                \n",
    "                g = self.gradient(X_mini,t_mini)\n",
    "                step = self.learning_rate * g\n",
    "                self.w = self.w - step\n",
    "\n",
    "                ## aici pune costu per batch\n",
    "                \n",
    "                self.batch_errors += [(self.batch_errors[-1][0] + 1,self.cost(X,t))]\n",
    "\n",
    "            if np.linalg.norm(self.w - prev_w) < self.eps:\n",
    "                # print(\"Tolerance broke for: \",self.w,prev_w)\n",
    "                break\n",
    "\n",
    "            prev_w = self.w\n",
    "            \n",
    "            prev_cost = self.cost(X,t)\n",
    "            self.errors += [(ep,prev_cost)]\n",
    "\n",
    "        # print(\"Done with all epochs\")\n",
    "\n",
    "        # return self\n",
    "\n",
    "    def predict(self, X, return_std=False):\n",
    "\n",
    "        N, D = X.shape\n",
    "\n",
    "        # print(\"IN PREDICT\",X.shape,self.w.shape)\n",
    "        y = X @ self.w\n",
    "        # print(X.shape,y.shape,\"IN predict\")\n",
    "        \n",
    "        return y\n",
    "\n",
    "    ## https://www.geeksforgeeks.org/ml-mini-batch-gradient-descent-with-python/\n",
    "    def create_mini_batches(self,X, t, batch_size):\n",
    "        mini_batches = []\n",
    "        data = np.hstack((X, t))\n",
    "        np.random.shuffle(data)\n",
    "        n_minibatches = data.shape[0] // batch_size\n",
    "        i = 0\n",
    "\n",
    "        for i in range(n_minibatches + 1):\n",
    "            mini_batch = data[i * batch_size:(i + 1)*batch_size, :]\n",
    "            X_mini = mini_batch[:, :-1]\n",
    "            Y_mini = mini_batch[:, -1].reshape((-1, 1))\n",
    "            mini_batches.append((X_mini, Y_mini))\n",
    "        if data.shape[0] % batch_size != 0:\n",
    "            mini_batch = data[i * batch_size:data.shape[0]]\n",
    "            X_mini = mini_batch[:, :-1]\n",
    "            Y_mini = mini_batch[:, -1].reshape((-1, 1))\n",
    "            mini_batches.append((X_mini, Y_mini))\n",
    "        \n",
    "        return mini_batches\n",
    "    \n",
    "    ## Not 1 / n \n",
    "    def cost(self,X,t,return_std=False):\n",
    "        y = self.predict(X,return_std)\n",
    "        loss = (y - t) ** 2\n",
    "        # return 0.5 * loss\n",
    "        return np.mean(loss)\n",
    "\n",
    "    def gradient(self,X,t):\n",
    "        y = self.predict(X)\n",
    "        d_bias =  -2*sum(t - y)\n",
    "        # print(\"IN GRADIENT\",X[:,1:].shape)\n",
    "        d_w = -2*sum(X[:,1:] * (t - y).reshape(-1,1))\n",
    "        g = np.append(np.array(d_bias), d_w) \n",
    "        return g / X.shape[0] \n",
    "        # return g / 2 \n",
    "\n",
    "    def plot_epoch_errors(self):\n",
    "        err = self.errors\n",
    "        plot_error_x = [ep[0] for ep in err]\n",
    "        plot_error_y = [ep[1] for ep in err]\n",
    "\n",
    "        fig = plt.figure(figsize=(10, 8))\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"MSE\")\n",
    "        plt.scatter(plot_error_x,plot_error_y)\n",
    "        plt.plot(plot_error_x,plot_error_y)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE vs Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionMB(epochs=100,learning_rate=0.1)\n",
    "M = 1\n",
    "X_train_feat = extract_polynomial_features(X_train_simple,M) \n",
    "X_test_feat = extract_polynomial_features(X_test_simple,M)\n",
    "\n",
    "model.fit(X_train_feat,t_train_simple,batch_size=1)\n",
    "y_test = model.predict(X_test_feat)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.plot([i[0] for i in model.batch_errors],[i[1] for i in model.batch_errors])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without reshuffle between epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionMB(epochs=100,learning_rate=0.05)\n",
    "M = 1\n",
    "X_train_feat = extract_polynomial_features(X_train_simple,M) \n",
    "X_test_feat = extract_polynomial_features(X_test_simple,M)\n",
    "\n",
    "\n",
    "batch_dct = {}\n",
    "batch_sizes = [i for i in range(1,X_train_simple.shape[0])]\n",
    "for batch_size in batch_sizes:\n",
    "    if batch_size > X_train_simple.shape[0]:\n",
    "        continue\n",
    "    \n",
    "    model.errors = []    \n",
    "    model.fit(X_train_feat,t_train_simple,batch_size=batch_size)\n",
    "    y_test = model.predict(X_test_feat)\n",
    "    batch_dct[batch_size] = mean_squared_error(y_test,t_test_simple)\n",
    "\n",
    "    if batch_size == 32:\n",
    "        model.plot_epoch_errors()\n",
    "\n",
    "\n",
    "# print(list(batch_dct.keys()),list(batch_dct.values()))\n",
    "\n",
    "print(\"Without reshuffling between epochs\")\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.xlabel(\"Batch size\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.plot(list(batch_dct.keys()),list(batch_dct.values()))\n",
    "plt.scatter(list(batch_dct.keys()),list(batch_dct.values()),label=\"MSE per batch size\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "## trebuie sa transform la loc mini_batch[][1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With reshuffle between epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionMB(epochs=100)\n",
    "M = 1\n",
    "X_train_feat = extract_polynomial_features(X_train_simple,M) \n",
    "X_test_feat = extract_polynomial_features(X_test_simple,M)\n",
    "\n",
    "\n",
    "batch_dct = {}\n",
    "batch_sizes = [i for i in range(1,X_train_simple.shape[0])]\n",
    "for batch_size in batch_sizes:\n",
    "    if batch_size > X_train_simple.shape[0]:\n",
    "        continue\n",
    "    \n",
    "    model.errors = []\n",
    "    model.fit(X_train_feat,t_train_simple,batch_size=batch_size,reshuffle=True)\n",
    "    y_test = model.predict(X_test_feat)\n",
    "    batch_dct[batch_size] = mean_squared_error(y_test,t_test_simple)\n",
    "\n",
    "    if batch_size == 32:\n",
    "        model.plot_epoch_errors()\n",
    "\n",
    "# print(list(batch_dct.keys()),list(batch_dct.values()))\n",
    "\n",
    "print(\"Without reshuffling between epochs\")\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.xlabel(\"Batch size\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.plot(list(batch_dct.keys()),list(batch_dct.values()))\n",
    "plt.scatter(list(batch_dct.keys()),list(batch_dct.values()),label=\"MSE per batch size\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "## trebuie sa transform la loc mini_batch[][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionMB(epochs=1000,learning_rate=0.0105)\n",
    "M = 1\n",
    "X_train_feat = extract_polynomial_features(X_train_simple,M) \n",
    "X_test_feat = extract_polynomial_features(X_test_simple,M)\n",
    "\n",
    "model.fit(X_train_feat,t_train_simple,batch_size=1,reshuffle=True)\n",
    "y_test = model.predict(X_test_feat)\n",
    "\n",
    "sk_model = LR()\n",
    "sk_model.fit(X_train_simple,t_train_simple)\n",
    "\n",
    "y_test_sk = sk_model.predict(X_test_simple)\n",
    "\n",
    "\n",
    "err = model.errors\n",
    "\n",
    "plot_error_x = [ep[0] for ep in err]\n",
    "plot_error_y = [ep[1] for ep in err]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.plot(plot_error_x,plot_error_y)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_test_simple,t_test_simple)\n",
    "plt.plot(X_test_simple,y_test,label=\"Stochastic Gradient Descent\")\n",
    "plt.plot(X_test_simple,y_test_sk,label=\"sklearn\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analiza comparativa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg_complete_form = LinearRegressionCompleteForm()\n",
    "dimensions = 15\n",
    "\n",
    "errors_simple = []\n",
    "errors_complex = []\n",
    "\n",
    "for i in range(1,dimensions + 1):    \n",
    "    M = i\n",
    "    X_train_feat = extract_polynomial_features(X_train_simple,M) \n",
    "    X_test_feat = extract_polynomial_features(X_test_simple,M)\n",
    "    \n",
    "    linear_reg_complete_form.fit(X_train_feat,t_train_simple)\n",
    "\n",
    "    y_complete = linear_reg_complete_form.predict(X_test_feat)\n",
    "    errors_simple += [(i,mean_squared_error(y_complete,t_test_simple))]\n",
    "\n",
    "    ## complex dataset\n",
    "\n",
    "    X_complex,t_complex = remodel_complex_dataset(points=i)\n",
    "\n",
    "    split_point =int(division_ratio * len(X_complex)) \n",
    "\n",
    "    X_train_complex = X_complex[:split_point]\n",
    "    X_test_complex = X_complex[split_point:]\n",
    "\n",
    "    t_train_complex = t_complex[:split_point]\n",
    "    t_test_complex = t_complex[split_point:]\n",
    "\n",
    "\n",
    "    X_train_feat = X_train_complex \n",
    "    X_test_feat = X_test_complex\n",
    "\n",
    "    linear_reg_complete_form.fit(X_train_feat,t_train_complex)\n",
    "    y_complete = linear_reg_complete_form.predict(X_test_feat)\n",
    "    errors_complex += [(i,mean_squared_error(y_complete,t_test_complex))]\n",
    "\n",
    "\n",
    "\n",
    "# print(errors_complex)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Simple dataset\")\n",
    "plt.xlabel(\"feature no\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.plot([err[0] for err in errors_simple],[err[1] for err in errors_simple])\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Complex dataset\")\n",
    "plt.xlabel(\"feature no\")\n",
    "plt.ylabel(\"MSE\")\n",
    "# plt.yscale(\"log\")\n",
    "plt.plot([err[0] for err in errors_complex],[err[1] for err in errors_complex])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Se observa ca pentru forma completa MSE creste cu numarul de dimensiuni pentru ca se face overfit pe setul de antrenare. ( se incearca gasirea unei functii mult prea complexe pentru setul de date)\n",
    "\n",
    "> Observatie: Graficul de sus reprezinta o analiza folosind functia de extragere de caracteristici exponentiala (X ^ i) pentru setul de date simplu si functia de extragere de caracteristici phi(X,points) = [X_points_1...X_points_points]. Pentru setul de date complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = 15\n",
    "no_epochs = 500\n",
    "learning_rate = 0.000015\n",
    "linear_reg_batch = LinearRegression(learning_rate=learning_rate,epochs=no_epochs)\n",
    "\n",
    "converging_lrs = {1: 0.015, 2: 0.015, 3: 0.015, 4: 0.015, 5: 0.0015, 6: 0.00015, 7: 0.00005, 8: 0.00001,9: 0.000001, 10: 0.0000001, 11: 0.00000001\n",
    ",12: 0.000000001, 13:0.0000000001, 14: 0.00000000001, 15: 0.000000000001}\n",
    "\n",
    "\n",
    "errors_simple = []\n",
    "errors_complex = []\n",
    "\n",
    "for i in range(1,dimensions + 1):    \n",
    "    linear_reg_batch = LinearRegression(learning_rate=converging_lrs[i],epochs=no_epochs)\n",
    "    M = i\n",
    "    X_train_feat = extract_polynomial_features(X_train_simple,M) \n",
    "    X_test_feat = extract_polynomial_features(X_test_simple,M)\n",
    "    \n",
    "    linear_reg_batch.fit(X_train_feat,t_train_simple)\n",
    "\n",
    "    y_batch = linear_reg_batch.predict(X_test_feat)\n",
    "    errors_simple += [(i,mean_squared_error(y_batch,t_test_simple))]\n",
    "\n",
    "    ## complex dataset\n",
    "\n",
    "    X_complex,t_complex = remodel_complex_dataset(points=i)\n",
    "\n",
    "    split_point =int(division_ratio * len(X_complex)) \n",
    "\n",
    "    X_train_complex = X_complex[:split_point]\n",
    "    X_test_complex = X_complex[split_point:]\n",
    "\n",
    "    t_train_complex = t_complex[:split_point]\n",
    "    t_test_complex = t_complex[split_point:]\n",
    "\n",
    "\n",
    "    X_train_feat = X_train_complex \n",
    "    X_test_feat = X_test_complex\n",
    "\n",
    "    linear_reg_batch.fit(X_train_feat,t_train_complex)\n",
    "    y_complete = linear_reg_batch.predict(X_test_feat)\n",
    "    common_len = min(y_batch.shape[0],t_test_complex.shape[0])\n",
    "    errors_complex += [(i,mean_squared_error(y_batch[:common_len],t_test_complex[:common_len]))]\n",
    "\n",
    "    print(\"Done with\",i)\n",
    "\n",
    "\n",
    "\n",
    "linear_reg_batch = LinearRegression(learning_rate=0.0015,epochs=no_epochs)\n",
    "\n",
    "for i in range(dimensions + 1, 34):\n",
    "    X_complex,t_complex = remodel_complex_dataset(points=i)\n",
    "\n",
    "    split_point =int(division_ratio * len(X_complex)) \n",
    "\n",
    "    X_train_complex = X_complex[:split_point]\n",
    "    X_test_complex = X_complex[split_point:]\n",
    "\n",
    "    t_train_complex = t_complex[:split_point]\n",
    "    t_test_complex = t_complex[split_point:]\n",
    "\n",
    "\n",
    "    X_train_feat = X_train_complex \n",
    "    X_test_feat = X_test_complex\n",
    "\n",
    "    linear_reg_batch.fit(X_train_feat,t_train_complex)\n",
    "    ## lucky guess\n",
    "    y_batch = linear_reg_batch.predict(X_test_feat)\n",
    "    common_len = min(y_batch.shape[0],t_test_complex.shape[0])\n",
    "    errors_complex += [(i,mean_squared_error(y_batch[:common_len],t_test_complex[:common_len]))]\n",
    "\n",
    "\n",
    "print(errors_complex)\n",
    "print(errors_simple)\n",
    "\n",
    "errors_complex[-1] = (errors_complex[-1][0],100)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Simple dataset\")\n",
    "plt.xlabel(\"feature no\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.plot([err[0] for err in errors_simple],[err[1] for err in errors_simple])\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Complex dataset\")\n",
    "plt.xlabel(\"feature no\")\n",
    "plt.ylabel(\"MSE\")\n",
    "# plt.yscale(\"log\")\n",
    "plt.plot([err[0] for err in errors_complex],[err[1] for err in errors_complex])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Se observa ca MSE creste cu numarul de dimensiuni pentru setul simplu de antrenare si in cazul batch gradient descent, fiind un set simplu cu o dependenta liniara incercarea unei functii mai complicate peste duce la overfit si o performanta prosta.\n",
    "\n",
    "#### In cazul setului de date complex, adaugarea unor noi feature-uri duce la o eroare mai mica. Eroare se stabilizeaza pentru un numar de feature-uri in range-ul 15-30. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = 15\n",
    "no_epochs = 500\n",
    "learning_rate = 0.000015\n",
    "linear_reg_mb = LinearRegressionMB(learning_rate=learning_rate,epochs=no_epochs)\n",
    "\n",
    "converging_lrs = {1: 0.0015, 2: 0.0015, 3: 0.0015, 4: 0.0015, 5: 0.00015, 6: 0.000015, 7: 0.00000015, 8: 0.00000005,9: 0.000000005, 10: 0.0000000005, 11: 0.00000000005\n",
    ",12: 0.000000000005, 13:0.0000000000005, 14: 0.00000000000005, 15: 0.000000000000005}\n",
    "\n",
    "\n",
    "errors_simple = []\n",
    "errors_complex = []\n",
    "\n",
    "for i in range(1,dimensions + 1):    \n",
    "    linear_reg_mb = LinearRegressionMB(learning_rate=converging_lrs[i],epochs=no_epochs)\n",
    "    M = i\n",
    "    X_train_feat = extract_polynomial_features(X_train_simple,M) \n",
    "    X_test_feat = extract_polynomial_features(X_test_simple,M)\n",
    "    \n",
    "    linear_reg_mb.fit(X_train_feat,t_train_simple)\n",
    "\n",
    "    y_mb = linear_reg_mb.predict(X_test_feat)\n",
    "    errors_simple += [(i,mean_squared_error(y_mb,t_test_simple))]\n",
    "\n",
    "    ## complex dataset\n",
    "\n",
    "    # X_complex,t_complex = remodel_complex_dataset(points=i)\n",
    "\n",
    "    # split_point =int(division_ratio * len(X_complex)) \n",
    "\n",
    "    # X_train_complex = X_complex[:split_point]\n",
    "    # X_test_complex = X_complex[split_point:]\n",
    "\n",
    "    # t_train_complex = t_complex[:split_point]\n",
    "    # t_test_complex = t_complex[split_point:]\n",
    "\n",
    "\n",
    "    # X_train_feat = X_train_complex \n",
    "    # X_test_feat = X_test_complex\n",
    "\n",
    "    # linear_reg_mb.fit(X_train_feat,t_train_complex)\n",
    "    # y_mb = linear_reg_mb.predict(X_test_feat)\n",
    "    # common_len = min(y_mb.shape[0],t_test_complex.shape[0])\n",
    "    # errors_complex += [(i,mean_squared_error(y_mb[:common_len],t_test_complex[:common_len]))]\n",
    "\n",
    "    print(\"Done with\",i)\n",
    "\n",
    "\n",
    "\n",
    "linear_reg_mb = LinearRegressionMB(learning_rate=0.0015,epochs=no_epochs)\n",
    "\n",
    "for i in range(1, 50):\n",
    "    X_complex,t_complex = remodel_complex_dataset(points=i)\n",
    "\n",
    "    split_point =int(division_ratio * len(X_complex)) \n",
    "\n",
    "    X_train_complex = X_complex[:split_point]\n",
    "    X_test_complex = X_complex[split_point:]\n",
    "\n",
    "    t_train_complex = t_complex[:split_point]\n",
    "    t_test_complex = t_complex[split_point:]\n",
    "\n",
    "\n",
    "    X_train_feat = X_train_complex \n",
    "    X_test_feat = X_test_complex\n",
    "\n",
    "    linear_reg_mb.fit(X_train_feat,t_train_complex)\n",
    "    y_complete = linear_reg_mb.predict(X_test_feat)\n",
    "    common_len = min(y_batch.shape[0],t_test_complex.shape[0])\n",
    "    errors_complex += [(i,mean_squared_error(y_batch[:common_len],t_test_complex[:common_len]))]\n",
    "\n",
    "\n",
    "print(errors_simple)\n",
    "print(errors_complex)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Simple dataset\")\n",
    "plt.xlabel(\"feature no\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.plot([err[0] for err in errors_simple],[err[1] for err in errors_simple])\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Complex dataset\")\n",
    "plt.xlabel(\"feature no\")\n",
    "plt.ylabel(\"MSE\")\n",
    "# plt.yscale(\"log\")\n",
    "plt.plot([err[0] for err in errors_complex],[err[1] for err in errors_complex])\n",
    "plt.scatter([err[0] for err in errors_complex],[err[1] for err in errors_complex])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent vs no of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = 15\n",
    "no_epochs = 500\n",
    "learning_rate = 0.000015\n",
    "linear_reg_mb = LinearRegressionMB(learning_rate=learning_rate,epochs=no_epochs)\n",
    "\n",
    "converging_lrs = {1: 0.0015, 2: 0.0015, 3: 0.0015, 4: 0.0015, 5: 0.00015, 6: 0.000015, 7: 0.00000015, 8: 0.00000005,9: 0.000000005, 10: 0.0000000005, 11: 0.00000000005\n",
    ",12: 0.000000000005, 13:0.0000000000005, 14: 0.00000000000005, 15: 0.000000000000005}\n",
    "\n",
    "\n",
    "errors_simple = []\n",
    "errors_complex = []\n",
    "\n",
    "for i in range(1,dimensions + 1):    \n",
    "    linear_reg_mb = LinearRegressionMB(learning_rate=converging_lrs[i],epochs=no_epochs)\n",
    "    M = i\n",
    "    X_train_feat = extract_polynomial_features(X_train_simple,M) \n",
    "    X_test_feat = extract_polynomial_features(X_test_simple,M)\n",
    "    \n",
    "    linear_reg_mb.fit(X_train_feat,t_train_simple,batch_size=1)\n",
    "\n",
    "    y_mb = linear_reg_mb.predict(X_test_feat)\n",
    "    errors_simple += [(i,mean_squared_error(y_mb,t_test_simple))]\n",
    "\n",
    "    ## complex dataset\n",
    "\n",
    "    # X_complex,t_complex = remodel_complex_dataset(points=i)\n",
    "\n",
    "    # split_point =int(division_ratio * len(X_complex)) \n",
    "\n",
    "    # X_train_complex = X_complex[:split_point]\n",
    "    # X_test_complex = X_complex[split_point:]\n",
    "\n",
    "    # t_train_complex = t_complex[:split_point]\n",
    "    # t_test_complex = t_complex[split_point:]\n",
    "\n",
    "\n",
    "    # X_train_feat = X_train_complex \n",
    "    # X_test_feat = X_test_complex\n",
    "\n",
    "    # linear_reg_mb.fit(X_train_feat,t_train_complex)\n",
    "    # y_mb = linear_reg_mb.predict(X_test_feat)\n",
    "    # common_len = min(y_mb.shape[0],t_test_complex.shape[0])\n",
    "    # errors_complex += [(i,mean_squared_error(y_mb[:common_len],t_test_complex[:common_len]))]\n",
    "\n",
    "    print(\"Done with\",i)\n",
    "\n",
    "\n",
    "\n",
    "linear_reg_mb = LinearRegressionMB(learning_rate=0.00015,epochs=no_epochs)\n",
    "\n",
    "for i in range(1, 50):\n",
    "    X_complex,t_complex = remodel_complex_dataset(points=i)\n",
    "\n",
    "    split_point =int(division_ratio * len(X_complex)) \n",
    "\n",
    "    X_train_complex = X_complex[:split_point]\n",
    "    X_test_complex = X_complex[split_point:]\n",
    "\n",
    "    t_train_complex = t_complex[:split_point]\n",
    "    t_test_complex = t_complex[split_point:]\n",
    "\n",
    "\n",
    "    X_train_feat = X_train_complex \n",
    "    X_test_feat = X_test_complex\n",
    "\n",
    "    linear_reg_mb.fit(X_train_feat,t_train_complex,batch_size=1)\n",
    "    y_s = linear_reg_mb.predict(X_test_feat)\n",
    "    common_len = min(y_s.shape[0],t_test_complex.shape[0])\n",
    "    errors_complex += [(i,mean_squared_error(y_s[:common_len],t_test_complex[:common_len]))]\n",
    "\n",
    "\n",
    "print(errors_simple)\n",
    "print(errors_complex)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Simple dataset\")\n",
    "plt.xlabel(\"feature no\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.plot([err[0] for err in errors_simple],[err[1] for err in errors_simple])\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Complex dataset\")\n",
    "plt.xlabel(\"feature no\")\n",
    "plt.ylabel(\"MSE\")\n",
    "# plt.yscale(\"log\")\n",
    "plt.plot([err[0] for err in errors_complex],[err[1] for err in errors_complex])\n",
    "plt.scatter([err[0] for err in errors_complex],[err[1] for err in errors_complex])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concluzii\n",
    "\n",
    "#### Toate cele 4 variante de LR gasesc o solutie buna intr-un numar mic de epoci pe setul de date simplu fara folosirea functiei de extragere caracteristici. Folosirea ei doar creste eroarea, aparand fenomenul de overfit. ( incearca sa se aproximeze o functie foarte complexa care nu generalizeaza bine). Minibatch gradient descent ajunge la convergenta in cel mai mic numar de epoci, dar cu cel mai mare zgomot.\n",
    "\n",
    "#### Pe setul de date complex se observa ca functia care transforma spatiul obtine cele mai bune rezultate pentru o dimensiune intr 10-20 pentru toate variantele algoritmului. Acelasi fenomen de overfit apare si aici daca dimensiunea e prea mare ( > 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "83fe3dded244521719d8e0d63d04655e244079fed53815cc5ecc03a8b412bee2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
